{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Install and Import Dependencies"
      ],
      "metadata": {
        "id": "RLrEQX3GZV4x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tensorflow numpy matplotlib IPython"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s9owVtmNmjC0",
        "outputId": "f2b57ed6-4864-4934-e3d6-1cb11b18b1f6"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: IPython in /usr/local/lib/python3.11/dist-packages (7.34.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.1.24)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.25.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.70.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.12.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.55.8)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.8.2)\n",
            "Collecting jedi>=0.16 (from IPython)\n",
            "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from IPython) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from IPython) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.11/dist-packages (from IPython) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from IPython) (3.0.50)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from IPython) (2.18.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from IPython) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.11/dist-packages (from IPython) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from IPython) (4.9.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->IPython) (0.8.4)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.14.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->IPython) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->IPython) (0.2.13)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Downloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m35.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: jedi\n",
            "Successfully installed jedi-0.19.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ctmQaqlTJRMF"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import time\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "from tensorflow.keras.layers import Dense, Reshape, BatchNormalization, ReLU, Conv2DTranspose, Conv2D, LeakyReLU, Dropout, Flatten\n",
        "from tensorflow.keras.losses import BinaryCrossentropy\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from IPython import display"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Dataset"
      ],
      "metadata": {
        "id": "GrgRT7n4ZdMN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess Fashion MNIST dataset\n",
        "(train_images, _), (_, _) = fashion_mnist.load_data()\n",
        "train_images = train_images.reshape(train_images.shape[0], 28, 28, 1).astype('float32')\n",
        "train_images = (train_images - 127.5) / 127.5  # Normalize to [-1, 1]\n",
        "\n",
        "BUFFER_SIZE = 60000\n",
        "BATCH_SIZE = 256\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices(train_images).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
      ],
      "metadata": {
        "id": "UbHVD_ZBJW3r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef13ff5d-87cb-426a-9a87-e698e35ec7d9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "\u001b[1m29515/29515\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "\u001b[1m26421880/26421880\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "\u001b[1m5148/5148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "\u001b[1m4422102/4422102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generator Model Architecture"
      ],
      "metadata": {
        "id": "qQOdROqVZfhF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generator = Sequential([\n",
        "    Dense(7 * 7 * 256, use_bias = False, input_shape = (100,)),\n",
        "    BatchNormalization(),\n",
        "    ReLU(),\n",
        "    Reshape((7, 7, 256)),\n",
        "    Conv2DTranspose(128, (5, 5), strides = (1, 1), padding = 'same', use_bias = False),\n",
        "    BatchNormalization(),\n",
        "    ReLU(),\n",
        "    Conv2DTranspose(64, (5, 5), strides = (2, 2), padding = 'same', use_bias = False),\n",
        "    BatchNormalization(),\n",
        "    ReLU(),\n",
        "    Conv2DTranspose(1, (5, 5), strides = (2, 2), padding = 'same', use_bias = False, activation = 'tanh')\n",
        "])"
      ],
      "metadata": {
        "id": "FArPWZekJdIL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c583832-8ec9-4772-b769-86ccf19898a9"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Discriminator Model Architecture"
      ],
      "metadata": {
        "id": "jhRbeLyhZkZu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "discriminator = Sequential([\n",
        "    Conv2D(64, (5, 5), strides = (2, 2), padding = 'same', input_shape = [28, 28, 1]),\n",
        "    LeakyReLU(),\n",
        "    Dropout(0.3),\n",
        "    Conv2D(128, (5, 5), strides = (2, 2), padding = 'same'),\n",
        "    LeakyReLU(),\n",
        "    Dropout(0.3),\n",
        "    Flatten(),\n",
        "    Dense(1)\n",
        "])"
      ],
      "metadata": {
        "id": "Ym-nLADTJk60",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54707dcd-d6e3-462d-a7a4-b5ee6c03770a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining Losses"
      ],
      "metadata": {
        "id": "ygYgI-dhZzYx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cross_entropy = BinaryCrossentropy(from_logits = True)\n",
        "\n",
        "def discriminator_loss(real_output, fake_output):\n",
        "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
        "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
        "    return real_loss + fake_loss\n",
        "\n",
        "def generator_loss(fake_output):\n",
        "    return cross_entropy(tf.ones_like(fake_output), fake_output)"
      ],
      "metadata": {
        "id": "h795zxpkFPHs"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining Optimizers"
      ],
      "metadata": {
        "id": "6B7hN-LyZ10z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generator_optimizer = Adam(1e-4)\n",
        "discriminator_optimizer = Adam(1e-4)"
      ],
      "metadata": {
        "id": "N8s0bPk-FRDX"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parameters for Training"
      ],
      "metadata": {
        "id": "e1uCszV0Z5OX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Loop\n",
        "EPOCHS = 150\n",
        "NOISE_DIM = 100\n",
        "num_examples_to_generate = 16\n",
        "seed = tf.random.normal([num_examples_to_generate, NOISE_DIM])\n",
        "\n",
        "def train_step(images):\n",
        "    noise = tf.random.normal([BATCH_SIZE, NOISE_DIM])\n",
        "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "        generated_images = generator(noise, training = True)\n",
        "        real_output = discriminator(images, training = True)\n",
        "        fake_output = discriminator(generated_images, training = True)\n",
        "        gen_loss = generator_loss(fake_output)\n",
        "        disc_loss = discriminator_loss(real_output, fake_output)\n",
        "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
        "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
        "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
        "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
        "    return gen_loss, disc_loss"
      ],
      "metadata": {
        "id": "dv7cWkRYFTZ9"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generating Images"
      ],
      "metadata": {
        "id": "orOoiVNmZ74U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_and_save_images(model, epoch, test_input):\n",
        "    predictions = model(test_input, training = False)\n",
        "    fig = plt.figure(figsize = (4, 4))\n",
        "    for i in range(predictions.shape[0]):\n",
        "        plt.subplot(4, 4, i+1)\n",
        "        plt.imshow(predictions[i, :, :, 0] * 127.5 + 127.5, cmap = 'gray')\n",
        "        plt.axis('off')\n",
        "    plt.savefig(f'image_at_epoch_{epoch:04d}.png')\n",
        "    plt.close()"
      ],
      "metadata": {
        "id": "ODL6wr2eFVqr"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Training"
      ],
      "metadata": {
        "id": "1Sn-6N1DaGo8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(dataset, epochs):\n",
        "    for epoch in range(epochs):\n",
        "        start = time.time()\n",
        "        gen_losses, disc_losses = [], []\n",
        "        for image_batch in dataset:\n",
        "            gen_loss, disc_loss = train_step(image_batch)\n",
        "            gen_losses.append(gen_loss)\n",
        "            disc_losses.append(disc_loss)\n",
        "        avg_gen_loss = tf.reduce_mean(gen_losses)\n",
        "        avg_disc_loss = tf.reduce_mean(disc_losses)\n",
        "        print(f'Epoch {epoch+1}, Gen Loss: {avg_gen_loss:.4f}, Disc Loss: {avg_disc_loss:.4f}, Time: {time.time()-start:.2f} sec')\n",
        "        if (epoch + 1) % 5 == 0:\n",
        "            generate_and_save_images(generator, epoch + 1, seed)\n",
        "    return generator, discriminator"
      ],
      "metadata": {
        "id": "L-IxQpKUFY8S"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Fitting Model"
      ],
      "metadata": {
        "id": "4cpuiQw2aKp7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generator, discriminator = train(train_dataset, EPOCHS)\n",
        "generator.save('fashion_mnist_generator.keras')\n",
        "discriminator.save('fashion_mnist_discriminator.keras')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xrJ04eXGFbFr",
        "outputId": "6af4eeae-5f08-4187-b9ad-bce6e45eedae"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Gen Loss: 0.7277, Disc Loss: 1.0730, Time: 58.38 sec\n",
            "Epoch 2, Gen Loss: 1.0323, Disc Loss: 1.0704, Time: 81.91 sec\n",
            "Epoch 3, Gen Loss: 0.9985, Disc Loss: 1.1715, Time: 51.03 sec\n",
            "Epoch 4, Gen Loss: 1.0591, Disc Loss: 1.1022, Time: 49.68 sec\n",
            "Epoch 5, Gen Loss: 0.9618, Disc Loss: 1.2184, Time: 50.40 sec\n",
            "Epoch 6, Gen Loss: 0.8483, Disc Loss: 1.2545, Time: 50.00 sec\n",
            "Epoch 7, Gen Loss: 0.8451, Disc Loss: 1.2498, Time: 49.19 sec\n",
            "Epoch 8, Gen Loss: 0.9003, Disc Loss: 1.2288, Time: 81.91 sec\n",
            "Epoch 9, Gen Loss: 0.8512, Disc Loss: 1.3370, Time: 81.91 sec\n",
            "Epoch 10, Gen Loss: 0.9234, Disc Loss: 1.2051, Time: 50.53 sec\n",
            "Epoch 11, Gen Loss: 0.9697, Disc Loss: 1.1875, Time: 50.45 sec\n",
            "Epoch 12, Gen Loss: 0.9370, Disc Loss: 1.1741, Time: 50.58 sec\n",
            "Epoch 13, Gen Loss: 1.0055, Disc Loss: 1.1235, Time: 49.70 sec\n",
            "Epoch 14, Gen Loss: 1.0162, Disc Loss: 1.1406, Time: 49.57 sec\n",
            "Epoch 15, Gen Loss: 1.0155, Disc Loss: 1.1042, Time: 49.56 sec\n",
            "Epoch 16, Gen Loss: 0.9856, Disc Loss: 1.1806, Time: 81.91 sec\n",
            "Epoch 17, Gen Loss: 1.0875, Disc Loss: 1.0414, Time: 50.29 sec\n",
            "Epoch 18, Gen Loss: 1.1311, Disc Loss: 1.0653, Time: 50.11 sec\n",
            "Epoch 19, Gen Loss: 1.1430, Disc Loss: 1.1222, Time: 50.01 sec\n",
            "Epoch 20, Gen Loss: 1.1634, Disc Loss: 1.0129, Time: 49.71 sec\n",
            "Epoch 21, Gen Loss: 1.1290, Disc Loss: 1.0495, Time: 49.70 sec\n",
            "Epoch 22, Gen Loss: 1.2246, Disc Loss: 0.9722, Time: 49.66 sec\n",
            "Epoch 23, Gen Loss: 1.1977, Disc Loss: 1.0117, Time: 49.38 sec\n",
            "Epoch 24, Gen Loss: 1.1859, Disc Loss: 1.0030, Time: 49.76 sec\n",
            "Epoch 25, Gen Loss: 1.4693, Disc Loss: 0.9138, Time: 49.84 sec\n",
            "Epoch 26, Gen Loss: 1.3574, Disc Loss: 0.9112, Time: 49.13 sec\n",
            "Epoch 27, Gen Loss: 1.5425, Disc Loss: 0.7895, Time: 49.98 sec\n",
            "Epoch 28, Gen Loss: 1.5665, Disc Loss: 0.8330, Time: 50.11 sec\n",
            "Epoch 29, Gen Loss: 1.5805, Disc Loss: 0.9387, Time: 49.66 sec\n",
            "Epoch 30, Gen Loss: 1.7977, Disc Loss: 0.7384, Time: 49.59 sec\n",
            "Epoch 31, Gen Loss: 1.5021, Disc Loss: 0.8065, Time: 49.97 sec\n",
            "Epoch 32, Gen Loss: 1.6699, Disc Loss: 0.8292, Time: 49.89 sec\n",
            "Epoch 33, Gen Loss: 1.6063, Disc Loss: 0.7773, Time: 49.72 sec\n",
            "Epoch 34, Gen Loss: 1.5797, Disc Loss: 0.8118, Time: 50.44 sec\n",
            "Epoch 35, Gen Loss: 1.5616, Disc Loss: 0.8508, Time: 49.88 sec\n",
            "Epoch 36, Gen Loss: 1.7484, Disc Loss: 0.7410, Time: 49.75 sec\n",
            "Epoch 37, Gen Loss: 1.6363, Disc Loss: 0.8614, Time: 49.72 sec\n",
            "Epoch 38, Gen Loss: 1.6492, Disc Loss: 0.8346, Time: 49.87 sec\n",
            "Epoch 39, Gen Loss: 1.5686, Disc Loss: 0.8708, Time: 49.72 sec\n",
            "Epoch 40, Gen Loss: 1.6327, Disc Loss: 0.8400, Time: 50.12 sec\n",
            "Epoch 41, Gen Loss: 1.6457, Disc Loss: 0.9404, Time: 81.91 sec\n",
            "Epoch 42, Gen Loss: 1.6515, Disc Loss: 0.7893, Time: 49.78 sec\n",
            "Epoch 43, Gen Loss: 1.5526, Disc Loss: 0.8710, Time: 49.69 sec\n",
            "Epoch 44, Gen Loss: 1.4696, Disc Loss: 0.9659, Time: 49.50 sec\n",
            "Epoch 45, Gen Loss: 1.5440, Disc Loss: 0.8825, Time: 49.25 sec\n",
            "Epoch 46, Gen Loss: 1.3422, Disc Loss: 1.0248, Time: 49.84 sec\n",
            "Epoch 47, Gen Loss: 1.4217, Disc Loss: 0.9403, Time: 49.89 sec\n",
            "Epoch 48, Gen Loss: 1.4070, Disc Loss: 0.9502, Time: 49.29 sec\n",
            "Epoch 49, Gen Loss: 1.3480, Disc Loss: 1.0103, Time: 49.92 sec\n",
            "Epoch 50, Gen Loss: 1.3604, Disc Loss: 0.9952, Time: 49.64 sec\n",
            "Epoch 51, Gen Loss: 1.4239, Disc Loss: 0.9749, Time: 49.75 sec\n",
            "Epoch 52, Gen Loss: 1.3565, Disc Loss: 1.0008, Time: 50.03 sec\n",
            "Epoch 53, Gen Loss: 1.3062, Disc Loss: 1.0373, Time: 49.11 sec\n",
            "Epoch 54, Gen Loss: 1.3008, Disc Loss: 1.0312, Time: 49.45 sec\n",
            "Epoch 55, Gen Loss: 1.2397, Disc Loss: 1.0450, Time: 49.70 sec\n",
            "Epoch 56, Gen Loss: 1.2204, Disc Loss: 1.0693, Time: 49.82 sec\n",
            "Epoch 57, Gen Loss: 1.2703, Disc Loss: 1.0153, Time: 49.76 sec\n",
            "Epoch 58, Gen Loss: 1.2115, Disc Loss: 1.0488, Time: 49.99 sec\n",
            "Epoch 59, Gen Loss: 1.2452, Disc Loss: 1.0680, Time: 81.91 sec\n",
            "Epoch 60, Gen Loss: 1.2354, Disc Loss: 1.0383, Time: 81.91 sec\n",
            "Epoch 61, Gen Loss: 1.2921, Disc Loss: 1.0667, Time: 49.97 sec\n",
            "Epoch 62, Gen Loss: 1.2753, Disc Loss: 1.0404, Time: 50.13 sec\n",
            "Epoch 63, Gen Loss: 1.2681, Disc Loss: 1.0835, Time: 49.81 sec\n",
            "Epoch 64, Gen Loss: 1.2031, Disc Loss: 1.1012, Time: 50.05 sec\n",
            "Epoch 65, Gen Loss: 1.1560, Disc Loss: 1.1233, Time: 49.91 sec\n",
            "Epoch 66, Gen Loss: 1.1919, Disc Loss: 1.0837, Time: 48.43 sec\n",
            "Epoch 67, Gen Loss: 1.1825, Disc Loss: 1.0909, Time: 49.12 sec\n",
            "Epoch 68, Gen Loss: 1.1941, Disc Loss: 1.0888, Time: 51.49 sec\n",
            "Epoch 69, Gen Loss: 1.1983, Disc Loss: 1.1082, Time: 50.78 sec\n",
            "Epoch 70, Gen Loss: 1.1743, Disc Loss: 1.1072, Time: 50.10 sec\n",
            "Epoch 71, Gen Loss: 1.1159, Disc Loss: 1.1642, Time: 81.91 sec\n",
            "Epoch 72, Gen Loss: 1.1658, Disc Loss: 1.1275, Time: 50.25 sec\n",
            "Epoch 73, Gen Loss: 1.1052, Disc Loss: 1.1478, Time: 50.37 sec\n",
            "Epoch 74, Gen Loss: 1.1007, Disc Loss: 1.1619, Time: 50.43 sec\n",
            "Epoch 75, Gen Loss: 1.0963, Disc Loss: 1.1404, Time: 50.26 sec\n",
            "Epoch 76, Gen Loss: 1.1119, Disc Loss: 1.1619, Time: 49.76 sec\n",
            "Epoch 77, Gen Loss: 1.1034, Disc Loss: 1.1802, Time: 49.58 sec\n",
            "Epoch 78, Gen Loss: 1.1643, Disc Loss: 1.1466, Time: 49.49 sec\n",
            "Epoch 79, Gen Loss: 1.1045, Disc Loss: 1.1721, Time: 49.61 sec\n",
            "Epoch 80, Gen Loss: 1.1156, Disc Loss: 1.1562, Time: 49.36 sec\n",
            "Epoch 81, Gen Loss: 1.0342, Disc Loss: 1.2115, Time: 49.79 sec\n",
            "Epoch 82, Gen Loss: 1.0558, Disc Loss: 1.1647, Time: 49.13 sec\n",
            "Epoch 83, Gen Loss: 1.0470, Disc Loss: 1.1963, Time: 49.68 sec\n",
            "Epoch 84, Gen Loss: 1.0045, Disc Loss: 1.2272, Time: 49.49 sec\n",
            "Epoch 85, Gen Loss: 1.0059, Disc Loss: 1.2077, Time: 48.94 sec\n",
            "Epoch 86, Gen Loss: 1.0099, Disc Loss: 1.2079, Time: 49.10 sec\n",
            "Epoch 87, Gen Loss: 1.0073, Disc Loss: 1.2081, Time: 49.03 sec\n",
            "Epoch 88, Gen Loss: 1.0440, Disc Loss: 1.1934, Time: 49.22 sec\n",
            "Epoch 89, Gen Loss: 1.0269, Disc Loss: 1.2369, Time: 48.80 sec\n",
            "Epoch 90, Gen Loss: 1.0305, Disc Loss: 1.2186, Time: 49.30 sec\n",
            "Epoch 91, Gen Loss: 1.0157, Disc Loss: 1.2221, Time: 48.79 sec\n",
            "Epoch 92, Gen Loss: 0.9971, Disc Loss: 1.2385, Time: 49.67 sec\n",
            "Epoch 93, Gen Loss: 0.9930, Disc Loss: 1.2470, Time: 48.99 sec\n",
            "Epoch 94, Gen Loss: 1.0083, Disc Loss: 1.2048, Time: 49.08 sec\n",
            "Epoch 95, Gen Loss: 0.9731, Disc Loss: 1.2518, Time: 81.91 sec\n",
            "Epoch 96, Gen Loss: 1.0073, Disc Loss: 1.2283, Time: 49.38 sec\n",
            "Epoch 97, Gen Loss: 0.9971, Disc Loss: 1.2485, Time: 48.80 sec\n",
            "Epoch 98, Gen Loss: 0.9676, Disc Loss: 1.2294, Time: 49.01 sec\n",
            "Epoch 99, Gen Loss: 0.9366, Disc Loss: 1.2645, Time: 81.91 sec\n",
            "Epoch 100, Gen Loss: 0.9403, Disc Loss: 1.2481, Time: 56.80 sec\n",
            "Epoch 101, Gen Loss: 0.9537, Disc Loss: 1.2724, Time: 57.50 sec\n",
            "Epoch 102, Gen Loss: 0.9832, Disc Loss: 1.2285, Time: 81.91 sec\n",
            "Epoch 103, Gen Loss: 0.9644, Disc Loss: 1.2330, Time: 81.91 sec\n",
            "Epoch 104, Gen Loss: 0.9504, Disc Loss: 1.2471, Time: 49.10 sec\n",
            "Epoch 105, Gen Loss: 0.9609, Disc Loss: 1.2348, Time: 49.17 sec\n",
            "Epoch 106, Gen Loss: 0.9795, Disc Loss: 1.2203, Time: 49.63 sec\n",
            "Epoch 107, Gen Loss: 0.9701, Disc Loss: 1.2460, Time: 49.70 sec\n",
            "Epoch 108, Gen Loss: 0.9429, Disc Loss: 1.2723, Time: 49.60 sec\n",
            "Epoch 109, Gen Loss: 0.9631, Disc Loss: 1.2428, Time: 49.65 sec\n",
            "Epoch 110, Gen Loss: 0.9671, Disc Loss: 1.2443, Time: 49.60 sec\n",
            "Epoch 111, Gen Loss: 0.9449, Disc Loss: 1.2506, Time: 49.17 sec\n",
            "Epoch 112, Gen Loss: 0.9407, Disc Loss: 1.2669, Time: 50.23 sec\n",
            "Epoch 113, Gen Loss: 0.9145, Disc Loss: 1.2885, Time: 49.62 sec\n",
            "Epoch 114, Gen Loss: 0.9371, Disc Loss: 1.2497, Time: 49.66 sec\n",
            "Epoch 115, Gen Loss: 0.9358, Disc Loss: 1.2743, Time: 48.91 sec\n",
            "Epoch 116, Gen Loss: 0.9387, Disc Loss: 1.2775, Time: 50.44 sec\n",
            "Epoch 117, Gen Loss: 0.9457, Disc Loss: 1.2424, Time: 49.76 sec\n",
            "Epoch 118, Gen Loss: 0.9539, Disc Loss: 1.2739, Time: 49.34 sec\n",
            "Epoch 119, Gen Loss: 0.9457, Disc Loss: 1.2585, Time: 49.55 sec\n",
            "Epoch 120, Gen Loss: 0.9329, Disc Loss: 1.2507, Time: 50.10 sec\n",
            "Epoch 121, Gen Loss: 0.9194, Disc Loss: 1.2775, Time: 50.75 sec\n",
            "Epoch 122, Gen Loss: 0.9243, Disc Loss: 1.2843, Time: 50.54 sec\n",
            "Epoch 123, Gen Loss: 0.9174, Disc Loss: 1.2780, Time: 50.64 sec\n",
            "Epoch 124, Gen Loss: 0.8981, Disc Loss: 1.2786, Time: 50.12 sec\n",
            "Epoch 125, Gen Loss: 0.9013, Disc Loss: 1.2812, Time: 50.18 sec\n",
            "Epoch 126, Gen Loss: 0.9126, Disc Loss: 1.2761, Time: 50.55 sec\n",
            "Epoch 127, Gen Loss: 0.9215, Disc Loss: 1.2522, Time: 48.59 sec\n",
            "Epoch 128, Gen Loss: 0.9178, Disc Loss: 1.2839, Time: 81.91 sec\n",
            "Epoch 129, Gen Loss: 0.9062, Disc Loss: 1.2968, Time: 48.24 sec\n",
            "Epoch 130, Gen Loss: 0.9284, Disc Loss: 1.2680, Time: 48.28 sec\n",
            "Epoch 131, Gen Loss: 0.9167, Disc Loss: 1.2928, Time: 49.59 sec\n",
            "Epoch 132, Gen Loss: 0.9324, Disc Loss: 1.2586, Time: 48.95 sec\n",
            "Epoch 133, Gen Loss: 0.9151, Disc Loss: 1.2940, Time: 48.84 sec\n",
            "Epoch 134, Gen Loss: 0.8930, Disc Loss: 1.2889, Time: 48.72 sec\n",
            "Epoch 135, Gen Loss: 0.8846, Disc Loss: 1.2735, Time: 48.34 sec\n",
            "Epoch 136, Gen Loss: 0.8810, Disc Loss: 1.2791, Time: 48.16 sec\n",
            "Epoch 137, Gen Loss: 0.8829, Disc Loss: 1.3007, Time: 48.93 sec\n",
            "Epoch 138, Gen Loss: 0.9025, Disc Loss: 1.2853, Time: 48.33 sec\n",
            "Epoch 139, Gen Loss: 0.9012, Disc Loss: 1.2887, Time: 47.93 sec\n",
            "Epoch 140, Gen Loss: 0.9241, Disc Loss: 1.2565, Time: 81.91 sec\n",
            "Epoch 141, Gen Loss: 0.9276, Disc Loss: 1.2594, Time: 49.87 sec\n",
            "Epoch 142, Gen Loss: 0.9451, Disc Loss: 1.2600, Time: 49.49 sec\n",
            "Epoch 143, Gen Loss: 0.8962, Disc Loss: 1.2812, Time: 81.91 sec\n",
            "Epoch 144, Gen Loss: 0.8966, Disc Loss: 1.3018, Time: 49.99 sec\n",
            "Epoch 145, Gen Loss: 0.8843, Disc Loss: 1.2818, Time: 81.91 sec\n",
            "Epoch 146, Gen Loss: 0.9035, Disc Loss: 1.2861, Time: 81.91 sec\n",
            "Epoch 147, Gen Loss: 0.8787, Disc Loss: 1.3005, Time: 48.94 sec\n",
            "Epoch 148, Gen Loss: 0.9060, Disc Loss: 1.2645, Time: 48.85 sec\n",
            "Epoch 149, Gen Loss: 0.9202, Disc Loss: 1.2848, Time: 48.68 sec\n",
            "Epoch 150, Gen Loss: 0.8780, Disc Loss: 1.3230, Time: 48.92 sec\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5Fp1M-vNFdRu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}